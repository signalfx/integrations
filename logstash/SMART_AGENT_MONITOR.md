
<!--- Generated by to-integrations-repo script in Smart Agent repo, DO NOT MODIFY HERE --->

### INSTALLATION

This integration is part of the [SignalFx Smart Agent](https://github.com/signalfx/integrations/tree/master/signalfx-agent)[](sfx_link:signalfx-agent)
as the `logstash`, `logstash-tcp` monitors. You should first deploy the Smart Agent to the
same host as the service you want to monitor, and then continue with the
configuration instructions below.

<!--- GENERATED BY (This comment exists for maintaining compatibility with to-product-docs) --->

## Description

This integration primarily consists of the Smart Agent monitors `logstash`, `logstash-tcp`.
Below is an overview of the monitors.

### Smart Agent Monitors

#### logstash monitor
Monitors the health and performance of Logstash deployments through 
Logstash's [Monitoring APIs](https://www.elastic.co/guide/en/logstash/current/monitoring-logstash.html).

#### logstash-tcp monitor
Fetches events from the [logstash tcp output
plugin](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-tcp.html)
operating in either `server` or `client` mode and converts them to SignalFx
datapoints.  It is meant to be used in conjunction with the Logstash
[Metrics filter
plugin](https://www.elastic.co/guide/en/logstash/current/plugins-filters-metrics.html)
that turns events into metrics.

You can only use auto-discovery when this monitor is in `client` mode.

### Example Logstash Config

This is a somewhat contrived example that shows the use of both `timer` and
`meter` metrics from the Logstash Metrics filter plugin:

```
input {
  file {
    path => "/var/log/auth.log"
    start_position => "beginning"
    tags => ["auth_log"]
  }

  # A contrived file that contains timing messages
  file {
    path => "/var/log/durations.log"
    tags => ["duration_log"]
    start_position => "beginning"
  }
}

filter {
  if "duration_log" in [tags] {
    dissect {
      mapping => {
        "message" => "Processing took %{duration} seconds"
      }
      convert_datatype => {
        "duration" => "float"
      }
    }
    if "_dissectfailure" not in [tags] { # Filter out bad events
      metrics {
        timer => { "process_time" => "%{duration}" }
        flush_interval => 10
        # This makes the timing stats pertain to only the previous 5 minutes
        # instead of since Logstash last started.
        clear_interval => 300
        add_field => {"type" => "processing"}
        add_tag => "metric"
      }
    }
  }
  # Count the number of logins via SSH from /var/log/auth.log
  if "auth_log" in [tags] and [message] =~ /sshd.*session opened/ {
    metrics {
      # This determines how often metric events will be sent to the agent, and
      # thus how often datapoints will be emitted.
      flush_interval => 10
      # The name of the meter will be used to construct the name of the metric
      # in SignalFx.  For this example, a datapoint called `logins.count` would
      # be generated.
      meter => "logins"
      add_tag => "metric"
    }
  }
}

output {
  # This can be helpful to debug
  stdout { codec => rubydebug }

  if "metric" in [tags] {
    tcp {
      port => 8900
      # The agent will connect to Logstash
      mode => "server"
      # Needs to be '0.0.0.0' if running in a container.
      host => "127.0.0.1"
    }
  }
}
```
Once Logstash is configured with the above configuration. The logstash-tcp monitor
will collect `logins.count` and `process_time.<timer_field>`. See config options
for default values.


## Configuration
#### logstash monitor
To activate this monitor in the Smart Agent, add the following to your
agent config:

```
monitors:  # All monitor config goes under this key
 - type: logstash
   ...  # Additional config
```

**For a list of monitor options that are common to all monitors, see [Common
Configuration](https://github.com/signalfx/signalfx-agent/tree/main/docs/monitors/../monitor-config.md#common-configuration).**


| Config option | Required | Type | Description |
| --- | --- | --- | --- |
| `host` | no | `string` | The hostname of Logstash monitoring API (**default:** `127.0.0.1`) |
| `port` | no | `integer` | The port number of Logstash monitoring API (**default:** `9600`) |
| `useHTTPS` | no | `bool` | If true, the agent will connect to the host using HTTPS instead of plain HTTP. (**default:** `false`) |
| `timeoutSeconds` | no | `integer` | The maximum amount of time to wait for API requests (**default:** `5`) |
#### logstash-tcp monitor
To activate this monitor in the Smart Agent, add the following to your
agent config:

```
monitors:  # All monitor config goes under this key
 - type: logstash-tcp
   ...  # Additional config
```

**For a list of monitor options that are common to all monitors, see [Common
Configuration](https://github.com/signalfx/signalfx-agent/tree/main/docs/monitors/../monitor-config.md#common-configuration).**


| Config option | Required | Type | Description |
| --- | --- | --- | --- |
| `host` | **yes** | `string` | If `mode: server`, the local IP address to listen on.  If `mode: client`, the Logstash host/ip to connect to. |
| `port` | no | `integer` | If `mode: server`, the local port to listen on.  If `mode: client`, the port of the Logstash TCP output plugin.  If port is `0`, a random listening port is assigned by the kernel. (**default:** `0`) |
| `mode` | no | `string` | Whether to act as a `server` or `client`.  The corresponding setting in the Logtash `tcp` output plugin should be set to the opposite of this. (**default:** `client`) |
| `desiredTimerFields` | no | `list of strings` |  (**default:** `[mean, max, p99, count]`) |
| `reconnectDelay` | no | `integer` | How long to wait before reconnecting if the TCP connection cannot be made or after it gets broken. (**default:** `5s`) |
| `debugEvents` | no | `bool` | If true, events received from Logstash will be dumped to the agent's stdout in deserialized form (**default:** `false`) |


## Metrics

Metrics that are categorized as
[container/host](https://docs.splunk.com/observability/admin/subscription-usage/monitor-imm-billing-usage.html#about-custom-bundled-and-high-resolution-metrics)
(*default*) are ***in bold and italics*** in the list below.

These are the metrics available for this integration.
#### logstash monitor
#### Group events
All of the following metrics are part of the `events` metric group. All of
the non-default metrics below can be turned on by adding `events` to the
monitor config option `extraGroups`:
 - ***`node.stats.events.events.duration_in_millis`*** (*cumulative*)<br>    Duration of events
 - ***`node.stats.events.events.filtered`*** (*cumulative*)<br>    Number of filtered events
 - ***`node.stats.events.events.in`*** (*cumulative*)<br>    Number of received events
 - ***`node.stats.events.events.out`*** (*cumulative*)<br>    Number of events sent out
 - ***`node.stats.events.events.queue_push_duration_in_millis`*** (*cumulative*)<br>    Duration that events waited in queue before being processed

#### Group hot_threads
All of the following metrics are part of the `hot_threads` metric group. All of
the non-default metrics below can be turned on by adding `hot_threads` to the
monitor config option `extraGroups`:
 - `node.hot_threads.hot_threads.busiest_threads` (*gauge*)<br>
 - `node.hot_threads.hot_threads.threads.percent_of_cpu_time` (*gauge*)<br>

#### Group jvm
All of the following metrics are part of the `jvm` metric group. All of
the non-default metrics below can be turned on by adding `jvm` to the
monitor config option `extraGroups`:
 - `node.stats.jvm.jvm.gc.collectors.old.collection_count` (*cumulative*)<br>    Total number of garbage collection events
 - `node.stats.jvm.jvm.gc.collectors.old.collection_time_in_millis` (*cumulative*)<br>    Amount of time spent garbage collecting in milliseconds
 - `node.stats.jvm.jvm.gc.collectors.young.collection_count` (*cumulative*)<br>    Total number of garbage collection events
 - `node.stats.jvm.jvm.gc.collectors.young.collection_time_in_millis` (*cumulative*)<br>    Amount of time spent garbage collecting in milliseconds
 - ***`node.stats.jvm.jvm.mem.heap_committed_in_bytes`*** (*gauge*)<br>    Total heap committed by the process
 - ***`node.stats.jvm.jvm.mem.heap_max_in_bytes`*** (*gauge*)<br>    Max memory being used
 - ***`node.stats.jvm.jvm.mem.heap_used_in_bytes`*** (*gauge*)<br>    Total heap used
 - `node.stats.jvm.jvm.mem.heap_used_percent` (*gauge*)<br>    Total heap used in percentage
 - `node.stats.jvm.jvm.mem.non_heap_committed_in_bytes` (*gauge*)<br>    Total non-heap memory committed by the process
 - ***`node.stats.jvm.jvm.mem.non_heap_used_in_bytes`*** (*gauge*)<br>    Total non-heap memory used
 - `node.stats.jvm.jvm.mem.pools.old.committed_in_bytes` (*gauge*)<br>    Memory guaranteed to be available to JVM non-heap by Old gen
 - `node.stats.jvm.jvm.mem.pools.old.max_in_bytes` (*gauge*)<br>    Max memory being used by Old Gen
 - `node.stats.jvm.jvm.mem.pools.old.peak_max_in_bytes` (*gauge*)<br>    Memory used by Old gen
 - `node.stats.jvm.jvm.mem.pools.old.peak_used_in_bytes` (*gauge*)<br>    Peak memory used by Old gen
 - `node.stats.jvm.jvm.mem.pools.old.used_in_bytes` (*gauge*)<br>    Memory being used by Old Gen
 - `node.stats.jvm.jvm.mem.pools.survivor.committed_in_bytes` (*gauge*)<br>    Memory guaranteed to be available to JVM non-heap by Survivor space
 - `node.stats.jvm.jvm.mem.pools.survivor.max_in_bytes` (*gauge*)<br>    Max memory being used by Survivor space
 - `node.stats.jvm.jvm.mem.pools.survivor.peak_max_in_bytes` (*gauge*)<br>    Memory used by Survivor space
 - `node.stats.jvm.jvm.mem.pools.survivor.peak_used_in_bytes` (*gauge*)<br>    Peak memory used by Survivor space
 - `node.stats.jvm.jvm.mem.pools.survivor.used_in_bytes` (*gauge*)<br>    Memory being used by Survivor space
 - `node.stats.jvm.jvm.mem.pools.young.committed_in_bytes` (*gauge*)<br>    Memory guaranteed to be available to JVM non-heap by Young gen
 - `node.stats.jvm.jvm.mem.pools.young.max_in_bytes` (*gauge*)<br>    Max memory being used by Young Gen
 - `node.stats.jvm.jvm.mem.pools.young.peak_max_in_bytes` (*gauge*)<br>    Memory used by Young gen
 - `node.stats.jvm.jvm.mem.pools.young.peak_used_in_bytes` (*gauge*)<br>    Peak memory used by Young gen
 - `node.stats.jvm.jvm.mem.pools.young.used_in_bytes` (*gauge*)<br>    Memory being used by Young Gen
 - ***`node.stats.jvm.jvm.threads.count`*** (*gauge*)<br>    Number of JVM threads
 - ***`node.stats.jvm.jvm.threads.peak_count`*** (*gauge*)<br>    Highest number of JVM threads
 - `node.stats.jvm.jvm.uptime_in_millis` (*gauge*)<br>    Uptime length of JVM

#### Group os
All of the following metrics are part of the `os` metric group. All of
the non-default metrics below can be turned on by adding `os` to the
monitor config option `extraGroups`:
 - `node.os.os.available_processors` (*gauge*)<br>    Number of available processors
 - `node.stats.os.os.cgroup.cpu.cfs_period_micros` (*gauge*)<br>
 - `node.stats.os.os.cgroup.cpu.cfs_quota_micros` (*gauge*)<br>
 - `node.stats.os.os.cgroup.cpu.stat.number_of_elapsed_periods` (*cumulative*)<br>
 - `node.stats.os.os.cgroup.cpu.stat.number_of_times_throttled` (*cumulative*)<br>
 - `node.stats.os.os.cgroup.cpu.stat.time_throttled_nanos` (*cumulative*)<br>
 - `node.stats.os.os.cgroup.cpuacct.usage_nanos` (*gauge*)<br>

#### Group pipeline
All of the following metrics are part of the `pipeline` metric group. All of
the non-default metrics below can be turned on by adding `pipeline` to the
monitor config option `extraGroups`:
 - `node.pipelines.batch_delay` (*gauge*)<br>
 - `node.pipelines.batch_size` (*gauge*)<br>
 - `node.pipelines.workers` (*gauge*)<br>    Number of workers in pipelines
 - ***`node.stats.pipelines.events.duration_in_millis`*** (*cumulative*)<br>    Duration of events in pipelines
 - ***`node.stats.pipelines.events.filtered`*** (*cumulative*)<br>    Number of filtered events in pipelines
 - ***`node.stats.pipelines.events.in`*** (*cumulative*)<br>    Number of received events in pipelines
 - ***`node.stats.pipelines.events.out`*** (*cumulative*)<br>    Number of events sent out from pipelines
 - `node.stats.pipelines.events.queue_push_duration_in_millis` (*cumulative*)<br>    Duration that events waited in queue before being processed in pipelines
 - `node.stats.pipelines.plugins.codecs.decode.duration_in_millis` (*cumulative*)<br>    Duration of decode events in codec plugins
 - `node.stats.pipelines.plugins.codecs.decode.out` (*cumulative*)<br>    Number of decode events sent out from codecs
 - `node.stats.pipelines.plugins.codecs.decode.writes_in` (*cumulative*)<br>    Number of received decode events in codecs
 - `node.stats.pipelines.plugins.codecs.encode.duration_in_millis` (*cumulative*)<br>    Duration of encode events in codec plugins
 - `node.stats.pipelines.plugins.codecs.encode.writes_in` (*cumulative*)<br>    Number of received encode events in codecs
 - ***`node.stats.pipelines.plugins.filters.events.duration_in_millis`*** (*cumulative*)<br>    Duration of events in filter plugins
 - ***`node.stats.pipelines.plugins.filters.events.in`*** (*cumulative*)<br>    Number of received events in filters
 - ***`node.stats.pipelines.plugins.filters.events.out`*** (*cumulative*)<br>    Number of events sent out from filters
 - ***`node.stats.pipelines.plugins.inputs.events.out`*** (*cumulative*)<br>    Number of events sent out from inputs
 - `node.stats.pipelines.plugins.inputs.events.queue_push_duration_in_millis` (*cumulative*)<br>    Duration that events waited in queue before being processed in inputs
 - ***`node.stats.pipelines.plugins.outputs.events.duration_in_millis`*** (*cumulative*)<br>    Duration of events in output plugins
 - ***`node.stats.pipelines.plugins.outputs.events.in`*** (*cumulative*)<br>    Number of received events in output plugins
 - ***`node.stats.pipelines.plugins.outputs.events.out`*** (*cumulative*)<br>    Number of events sent out from output plugins
 - `node.stats.pipelines.queue.events_count` (*gauge*)<br>    Number of events waiting in queue
 - `node.stats.pipelines.queue.max_queue_size_in_bytes` (*gauge*)<br>    Max queue size in pipelines
 - `node.stats.pipelines.queue.queue_size_in_bytes` (*gauge*)<br>    Queue size in pipelines
 - `node.stats.pipelines.reloads.failures` (*cumulative*)<br>    Number of failed reloads
 - `node.stats.pipelines.reloads.successes` (*cumulative*)<br>    Number of successful reloads

#### Group plugins
All of the following metrics are part of the `plugins` metric group. All of
the non-default metrics below can be turned on by adding `plugins` to the
monitor config option `extraGroups`:
 - ***`node.plugins.total`*** (*gauge*)<br>    Number of plugins

#### Group process
All of the following metrics are part of the `process` metric group. All of
the non-default metrics below can be turned on by adding `process` to the
monitor config option `extraGroups`:
 - `node.stats.process.process.cpu.load_average.15m` (*gauge*)<br>    CPU Load average in 15 minutes
 - `node.stats.process.process.cpu.load_average.1m` (*gauge*)<br>    CPU Load average in 1 minute
 - `node.stats.process.process.cpu.load_average.5m` (*gauge*)<br>    CPU Load average in 5 minutes
 - ***`node.stats.process.process.cpu.percent`*** (*gauge*)<br>    CPU usage in percent
 - `node.stats.process.process.cpu.total_in_millis` (*cumulative*)<br>    Total CPU time (in milliseconds) used by the process on which the process is running
 - `node.stats.process.process.max_file_descriptors` (*gauge*)<br>    Number of opened file descriptors associated with the current process
 - `node.stats.process.process.mem.total_virtual_in_bytes` (*gauge*)<br>    Size of the virtual memory of this process
 - `node.stats.process.process.open_file_descriptors` (*gauge*)<br>    Number of currently open file descriptors
 - `node.stats.process.process.peak_open_file_descriptors` (*gauge*)<br>    Peak number of currently open file descriptors

#### Group reloads
All of the following metrics are part of the `reloads` metric group. All of
the non-default metrics below can be turned on by adding `reloads` to the
monitor config option `extraGroups`:
 - `node.stats.reloads.reloads.failures` (*cumulative*)<br>    Number of failed reloads
 - `node.stats.reloads.reloads.successes` (*cumulative*)<br>    Number of successful reloads

### Non-default metrics (version 4.7.0+)

**The following information applies to the agent version 4.7.0+ that has
`enableBuiltInFiltering: true` set on the top level of the agent config.**

To emit metrics that are not _default_, you can add those metrics in the
generic monitor-level `extraMetrics` config option.  Metrics that are derived
from specific configuration options that do not appear in the above list of
metrics do not need to be added to `extraMetrics`.

To see a list of metrics that will be emitted you can run `agent-status
monitors` after configuring this monitor in a running agent instance.

### Legacy non-default metrics (version < 4.7.0)

**The following information only applies to agent versions prior to 4.7.0. If
you have a newer agent and have set `enableBuiltInFiltering: true` at the top
level of your agent config, see the section above. See upgrade instructions in
[Old-style inclusion list filtering](https://github.com/signalfx/signalfx-agent/tree/main/docs/monitors/../legacy-filtering.md#old-style-inclusion-list-filtering).**

If you have a reference to the `whitelist.json` in your agent's top-level
`metricsToExclude` config option, and you want to emit metrics that are not in
that allow list, then you need to add an item to the top-level
`metricsToInclude` config option to override that allow list (see [Inclusion
filtering](https://github.com/signalfx/signalfx-agent/tree/main/docs/monitors/../legacy-filtering.md#inclusion-filtering).  Or you can just
copy the whitelist.json, modify it, and reference that in `metricsToExclude`.

